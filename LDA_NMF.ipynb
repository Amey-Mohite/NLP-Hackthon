{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amey\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Amey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Amey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Amey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "from string import punctuation\n",
    "custom = stop_words+list(punctuation)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amey\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (5,6,11,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"G:\\study\\Natural Language processing\\nlp_class\\Consumer_Complaints.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data[['Consumer complaint narrative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>562059</th>\n",
       "      <td>A charge was made by a company making false cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562662</th>\n",
       "      <td>This debt is beyond the Maryland Statute of Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563174</th>\n",
       "      <td>My personal belongings and vehicle were stolen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563593</th>\n",
       "      <td>This complaint is necessary to ensure the reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563760</th>\n",
       "      <td>I was taken to the hospital while   XXXX   XXX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Consumer complaint narrative\n",
       "562059  A charge was made by a company making false cl...\n",
       "562662  This debt is beyond the Maryland Statute of Li...\n",
       "563174  My personal belongings and vehicle were stolen...\n",
       "563593  This complaint is necessary to ensure the reco...\n",
       "563760  I was taken to the hospital while   XXXX   XXX..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=data1['Consumer complaint narrative'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_to_be_removed = list(stopwords.words(\"english\"))+list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepross(s):\n",
    "    words = s.lower()\n",
    "    #print(words)\n",
    "    words = word_tokenize(words)\n",
    "    #print(words)\n",
    "    words = [x for x in words if x not in stuff_to_be_removed]\n",
    "    #print(words)\n",
    "    words = [ls.stem(x) for x in words]\n",
    "    words = [\" \".join(words)]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[]\n",
    "for i in range(len(corpus)):\n",
    "    z= prepross(corpus[i])\n",
    "    final.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charg mad company mak fals claim produc also ad addit produc purchas bank hon disput charg absurd lat fee bal paid'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = [word for line in final for word in line.split()]\n",
    "text=[]\n",
    "for i in range(len(final)):\n",
    "    for word in final[i][0].split():\n",
    "        text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(final)\n",
    "new_df.columns = ['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>charg mad company mak fals claim produc also a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>debt beyond maryland statut limit illeg debt c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person belong vehic stol xxxx =my lock brok po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complaint necess ens recovery collect firm 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tak hospit xxxx xxxx xxxx xxxx xxxx xxxx xxxx ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Consumer complaint narrative\n",
       "0  charg mad company mak fals claim produc also a...\n",
       "1  debt beyond maryland statut limit illeg debt c...\n",
       "2  person belong vehic stol xxxx =my lock brok po...\n",
       "3  complaint necess ens recovery collect firm 's ...\n",
       "4  tak hospit xxxx xxxx xxxx xxxx xxxx xxxx xxxx ..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tfidf.fit_transform(new_df['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1295)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0][0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1=pd.DataFrame(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1957)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=3,n_iter=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=60,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.fit(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1295"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsa.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09280222,  0.00463801,  0.00882923, ...,  0.0013318 ,\n",
       "         0.00294172,  0.01088053],\n",
       "       [ 0.04350081, -0.00493068,  0.00417542, ...,  0.00027945,\n",
       "        -0.00860692,  0.01269007],\n",
       "       [ 0.1353849 ,  0.0065807 ,  0.03034708, ...,  0.00279956,\n",
       "        -0.00046507, -0.02004785]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02165775, 0.0375576 , 0.02673777])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"The amount of pollution is increasing day by day\",\n",
    "\"The concert was just great\", \"I love to see Gordon Ramsay cook\",\n",
    "\"Google is introducing a new technlogy\", \"AI robots are great example of great technology present today\",\n",
    "\"All of us were singing in the concert\", \"We have launch campaigns to stop pollution and global warming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The amount of pollution is increasing day by day',\n",
       " 'The concert was just great',\n",
       " 'I love to see Gordon Ramsay cook',\n",
       " 'Google is introducing a new technlogy',\n",
       " 'AI robots are great example of great technology present today',\n",
       " 'All of us were singing in the concert',\n",
       " 'We have launch campaigns to stop pollution and global warming']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [line.lower() for  line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-55db9a66fb31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tok' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 42)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=4,n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11696462486835726"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "c={}\n",
    "def abc(d,j):\n",
    "    d={}\n",
    "    for i in range(len(lsa.components_[j])):\n",
    "        d[words[i]] = lsa.components_[j][i]\n",
    "    d=sorted(d, key=d.get, reverse=True)\n",
    "    d=d[:10]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = abc(c,0)\n",
    "d1 = abc(c,1)\n",
    "d2 = abc(c,2)\n",
    "d3 = abc(c,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'the', 'concert', 'of', 'just', 'was', 'day', 'all', 'in', 'singing']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'pollution',\n",
       " 'day',\n",
       " 'is',\n",
       " 'cook',\n",
       " 'gordon',\n",
       " 'love',\n",
       " 'ramsay',\n",
       " 'see',\n",
       " 'and']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'google',\n",
       " 'introducing',\n",
       " 'new',\n",
       " 'technlogy',\n",
       " 'day',\n",
       " 'amount',\n",
       " 'by',\n",
       " 'increasing',\n",
       " 'of']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'google',\n",
       " 'introducing',\n",
       " 'new',\n",
       " 'technlogy',\n",
       " 'are',\n",
       " 'example',\n",
       " 'present',\n",
       " 'robots',\n",
       " 'technology']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0 ;\n",
      "('great', 0.4098416235534007)\n",
      "('the', 0.37872314722187916)\n",
      "('concert', 0.3574166811716594)\n",
      "('of', 0.2773738067325902)\n",
      "('just', 0.2598047527022688)\n",
      "('was', 0.2598047527022688)\n",
      "('day', 0.2063766936709453)\n",
      "('all', 0.1707732032489237)\n",
      "('in', 0.1707732032489237)\n",
      "('singing', 0.1707732032489237)\n",
      "\n",
      "Topic 1 ;\n",
      "('to', 0.3218414494137703)\n",
      "('pollution', 0.26410761357197976)\n",
      "('day', 0.24902077191893163)\n",
      "('is', 0.2290114925683949)\n",
      "('cook', 0.1940620278819667)\n",
      "('gordon', 0.1940620278819667)\n",
      "('love', 0.1940620278819667)\n",
      "('ramsay', 0.1940620278819667)\n",
      "('see', 0.1940620278819667)\n",
      "('and', 0.193658637462118)\n",
      "\n",
      "Topic 2 ;\n",
      "('is', 0.34424073742032224)\n",
      "('google', 0.29368746403512386)\n",
      "('introducing', 0.29368746403512386)\n",
      "('new', 0.29368746403512386)\n",
      "('technlogy', 0.29368746403512386)\n",
      "('day', 0.2420350065062839)\n",
      "('amount', 0.12101750325314209)\n",
      "('by', 0.12101750325314195)\n",
      "('increasing', 0.12101750325314195)\n",
      "('of', 0.0460701545850063)\n",
      "\n",
      "Topic 3 ;\n",
      "('great', 0.36097341569294356)\n",
      "('google', 0.21834424295923002)\n",
      "('introducing', 0.21834424295923002)\n",
      "('new', 0.21834424295923002)\n",
      "('technlogy', 0.21834424295923002)\n",
      "('are', 0.20230022314967963)\n",
      "('example', 0.20230022314967963)\n",
      "('present', 0.20230022314967963)\n",
      "('robots', 0.20230022314967963)\n",
      "('technology', 0.20230022314967963)\n"
     ]
    }
   ],
   "source": [
    "for i,y in enumerate(lsa.components_):\n",
    "    componentwords = zip(words, y)\n",
    "    sortedComponentwords = sorted(componentwords, key = lambda x: x[1],\n",
    "                                reverse=True)\n",
    "    sortedComponentwords = sortedComponentwords[:10]\n",
    "    print (\"\\nTopic\", i, \";\")\n",
    "    for x in sortedComponentwords:\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=3,n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.fit(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai',\n",
       " 'all',\n",
       " 'amount',\n",
       " 'and',\n",
       " 'are',\n",
       " 'by',\n",
       " 'campaigns',\n",
       " 'concert',\n",
       " 'cook',\n",
       " 'day',\n",
       " 'example',\n",
       " 'global',\n",
       " 'google',\n",
       " 'gordon',\n",
       " 'great',\n",
       " 'have',\n",
       " 'in',\n",
       " 'increasing',\n",
       " 'introducing',\n",
       " 'is',\n",
       " 'just',\n",
       " 'launch',\n",
       " 'love',\n",
       " 'new',\n",
       " 'of',\n",
       " 'pollution',\n",
       " 'present',\n",
       " 'ramsay',\n",
       " 'robots',\n",
       " 'see',\n",
       " 'singing',\n",
       " 'stop',\n",
       " 'technlogy',\n",
       " 'technology',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'us',\n",
       " 'warming',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0 ;\n",
      "('ai', 0.0928022218121584)\n",
      "('pollution', 0.05287045736891479)\n",
      "('us', 0.017688459454025994)\n",
      "('google', 0.017411505722448112)\n",
      "('day', 0.008985858938185123)\n",
      "('amount', 0.008829227700539236)\n",
      "('of', 0.008480530156183464)\n",
      "('new', 0.00837043943340014)\n",
      "('warming', 0.006871946761745564)\n",
      "('have', 0.006768216053919551)\n",
      "\n",
      "Topic 1 ;\n",
      "('ai', 0.04350080976948647)\n",
      "('us', 0.02079574274802273)\n",
      "('of', 0.014330022987497486)\n",
      "('google', 0.014089941601454634)\n",
      "('pollution', 0.01406729586217281)\n",
      "('today', 0.010350445676500277)\n",
      "('technology', 0.009508829837328921)\n",
      "('cook', 0.006897639224349192)\n",
      "('in', 0.006681990792328114)\n",
      "('present', 0.005740677323670909)\n",
      "\n",
      "Topic 2 ;\n",
      "('ai', 0.1353848960786917)\n",
      "('google', 0.05315963132700794)\n",
      "('warming', 0.03482486738108809)\n",
      "('amount', 0.03034708131442759)\n",
      "('in', 0.02156392137072954)\n",
      "('increasing', 0.016607119111663786)\n",
      "('see', 0.016607119111663786)\n",
      "('is', 0.012985154073591874)\n",
      "('by', 0.011258362712682459)\n",
      "('example', 0.011258362712682459)\n"
     ]
    }
   ],
   "source": [
    "for i,y in enumerate(lsa.components_):\n",
    "    componentwords = zip(words, y)\n",
    "    sortedComponentwords = sorted(componentwords, key = lambda x: x[1],\n",
    "                                reverse=True)\n",
    "    sortedComponentwords = sortedComponentwords[:10]\n",
    "    print (\"\\nTopic\", i, \";\")\n",
    "    for x in sortedComponentwords:\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t)>2] #remove words lesser than 2 in length\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] #lemmatize words\n",
    "    tokens = [t for t in tokens if t not in custom] #remove stopwords and punctuation\n",
    "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)] # remove digits\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [my_tokenizer(s) for s in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import dictionary\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(27 unique tokens: ['amount', 'day', 'increasing', 'pollution', 'concert']...)\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(text)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus = [id2word.doc2bow(a) for a in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 2), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(12, 1), (13, 1), (14, 1), (15, 1)],\n",
       " [(5, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(4, 1), (21, 1)],\n",
       " [(3, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=mycorpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5686642234772799\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type complex is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(data, kwds)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text/html'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     formatter.for_type(PreparedData,\n\u001b[1;32m--> 313\u001b[1;33m                        lambda data, kwds=kwargs: prepared_data_to_html(data, **kwds))\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_display.py\u001b[0m in \u001b[0;36mprepared_data_to_html\u001b[1;34m(data, d3_url, ldavis_url, ldavis_css_url, template_type, visid, use_http)\u001b[0m\n\u001b[0;32m    176\u001b[0m                            \u001b[0md3_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md3_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                            \u001b[0mldavis_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mldavis_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                            \u001b[0mvis_json\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                            ldavis_css_url=ldavis_css_url)\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mto_json\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m        \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNumPyEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m         **kw).encode(obj)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyLDAvis\\utils.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type complex is not JSON serializable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=                                 x                            y  topics  \\\n",
       "topic                                                                     \n",
       "5         (0.18801443699018403+0j)   (-0.019111898877925734+0j)       1   \n",
       "8        (-0.21413382303742434+0j)     (0.06324909302949697+0j)       2   \n",
       "19       (0.018041131365856142+0j)    (-0.26296288160287734+0j)       3   \n",
       "3        (-0.12654627758717757+0j)    (0.016969040635350055+0j)       4   \n",
       "9          (0.1336601552031702+0j)     (0.13465749118331224+0j)       5   \n",
       "2         (0.05204106756977736+0j)     (0.11268613755560936+0j)       6   \n",
       "14      (-0.003648335036027546+0j)  (-0.0032490701373545997+0j)       7   \n",
       "13     (-0.0036483350360275316+0j)    (-0.00324907013735462+0j)       8   \n",
       "12      (-0.003648335036027542+0j)   (-0.003249070137354622+0j)       9   \n",
       "11      (-0.003648335036027517+0j)  (-0.0032490701373546374+0j)      10   \n",
       "10      (-0.003648335036027518+0j)  (-0.0032490701373546387+0j)      11   \n",
       "18     (-0.0036483350360275307+0j)  (-0.0032490701373546244+0j)      12   \n",
       "15     (-0.0036483350360275316+0j)  (-0.0032490701373546387+0j)      13   \n",
       "7      (-0.0036483350360275394+0j)  (-0.0032490701373545884+0j)      14   \n",
       "6      (-0.0036483350360275403+0j)  (-0.0032490701373546105+0j)      15   \n",
       "16      (-0.003648335036027549+0j)   (-0.003249070137354613+0j)      16   \n",
       "4      (-0.0036483350360275355+0j)   (-0.003249070137354613+0j)      17   \n",
       "17      (-0.003648335036027549+0j)    (-0.00324907013735461+0j)      18   \n",
       "1      (-0.0036483350360275502+0j)  (-0.0032490701373546196+0j)      19   \n",
       "0      (-0.0036483350360275455+0j)    (-0.00324907013735461+0j)      20   \n",
       "\n",
       "       cluster       Freq  \n",
       "topic                      \n",
       "5            1  37.852783  \n",
       "8            1  14.443946  \n",
       "19           1  14.443937  \n",
       "3            1  11.388433  \n",
       "9            1   8.370687  \n",
       "2            1   5.431093  \n",
       "14           1   0.576366  \n",
       "13           1   0.576366  \n",
       "12           1   0.576366  \n",
       "11           1   0.576366  \n",
       "10           1   0.576366  \n",
       "18           1   0.576366  \n",
       "15           1   0.576366  \n",
       "7            1   0.576366  \n",
       "6            1   0.576366  \n",
       "16           1   0.576366  \n",
       "4            1   0.576366  \n",
       "17           1   0.576366  \n",
       "1            1   0.576366  \n",
       "0            1   0.576366  , topic_info=   Category      Freq         Term     Total  loglift  logprob\n",
       "5   Default  2.000000        great  2.000000  27.0000  27.0000\n",
       "1   Default  1.000000          day  1.000000  26.0000  26.0000\n",
       "3   Default  1.000000    pollution  1.000000  25.0000  25.0000\n",
       "4   Default  1.000000      concert  1.000000  24.0000  24.0000\n",
       "26  Default  1.000000      warming  1.000000  23.0000  23.0000\n",
       "23  Default  1.000000       global  1.000000  22.0000  22.0000\n",
       "17  Default  1.000000      present  1.000000  21.0000  21.0000\n",
       "16  Default  1.000000      example  1.000000  20.0000  20.0000\n",
       "20  Default  1.000000        today  1.000000  19.0000  19.0000\n",
       "22  Default  1.000000     campaign  1.000000  18.0000  18.0000\n",
       "18  Default  1.000000        robot  1.000000  17.0000  17.0000\n",
       "19  Default  1.000000   technology  1.000000  16.0000  16.0000\n",
       "24  Default  1.000000       launch  1.000000  15.0000  15.0000\n",
       "25  Default  1.000000         stop  1.000000  14.0000  14.0000\n",
       "11  Default  1.000000          see  1.000000  13.0000  13.0000\n",
       "10  Default  1.000000       ramsay  1.000000  12.0000  12.0000\n",
       "8   Default  1.000000       gordon  1.000000  11.0000  11.0000\n",
       "7   Default  1.000000         cook  1.000000  10.0000  10.0000\n",
       "9   Default  1.000000         love  1.000000   9.0000   9.0000\n",
       "0   Default  1.000000       amount  1.000000   8.0000   8.0000\n",
       "2   Default  1.000000   increasing  1.000000   7.0000   7.0000\n",
       "15  Default  0.000000    technlogy  0.000000   6.0000   6.0000\n",
       "14  Default  0.000000          new  0.000000   5.0000   5.0000\n",
       "12  Default  0.000000       google  0.000000   4.0000   4.0000\n",
       "13  Default  0.000000  introducing  0.000000   3.0000   3.0000\n",
       "6   Default  0.000000           wa  0.000000   2.0000   2.0000\n",
       "21  Default  0.000000      singing  0.000000   1.0000   1.0000\n",
       "26   Topic1  0.886309      warming  1.145519   0.7149  -2.6150\n",
       "24   Topic1  0.886309       launch  1.145519   0.7149  -2.6150\n",
       "23   Topic1  0.886309       global  1.145519   0.7149  -2.6150\n",
       "..      ...       ...          ...       ...      ...      ...\n",
       "1   Topic19  0.006831          day  1.757183  -0.3938  -3.2958\n",
       "3   Topic19  0.006831    pollution  1.873403  -0.4579  -3.2958\n",
       "5   Topic19  0.006831        great  2.605397  -0.7877  -3.2958\n",
       "21  Topic20  0.006831      singing  0.820206   0.3681  -3.2958\n",
       "6   Topic20  0.006831           wa  0.917190   0.2563  -3.2958\n",
       "13  Topic20  0.006831  introducing  0.982592   0.1875  -3.2958\n",
       "15  Topic20  0.006831    technlogy  0.982592   0.1875  -3.2958\n",
       "12  Topic20  0.006831       google  0.982592   0.1875  -3.2958\n",
       "14  Topic20  0.006831          new  0.982592   0.1875  -3.2958\n",
       "2   Topic20  0.006831   increasing  1.029298   0.1410  -3.2958\n",
       "0   Topic20  0.006831       amount  1.029298   0.1410  -3.2958\n",
       "9   Topic20  0.006831         love  1.029299   0.1410  -3.2958\n",
       "10  Topic20  0.006831       ramsay  1.029299   0.1410  -3.2958\n",
       "7   Topic20  0.006831         cook  1.029299   0.1410  -3.2958\n",
       "8   Topic20  0.006831       gordon  1.029299   0.1410  -3.2958\n",
       "11  Topic20  0.006831          see  1.029299   0.1410  -3.2958\n",
       "24  Topic20  0.006831       launch  1.145519   0.0340  -3.2958\n",
       "23  Topic20  0.006831       global  1.145519   0.0340  -3.2958\n",
       "17  Topic20  0.006831      present  1.145519   0.0340  -3.2958\n",
       "18  Topic20  0.006831        robot  1.145519   0.0340  -3.2958\n",
       "22  Topic20  0.006831     campaign  1.145519   0.0340  -3.2958\n",
       "25  Topic20  0.006831         stop  1.145519   0.0340  -3.2958\n",
       "20  Topic20  0.006831        today  1.145519   0.0340  -3.2958\n",
       "19  Topic20  0.006831   technology  1.145519   0.0340  -3.2958\n",
       "16  Topic20  0.006831      example  1.145519   0.0340  -3.2958\n",
       "26  Topic20  0.006831      warming  1.145519   0.0340  -3.2958\n",
       "4   Topic20  0.006831      concert  1.435982  -0.1919  -3.2958\n",
       "1   Topic20  0.006831          day  1.757183  -0.3938  -3.2958\n",
       "3   Topic20  0.006831    pollution  1.873403  -0.4579  -3.2958\n",
       "5   Topic20  0.006831        great  2.605397  -0.7877  -3.2958\n",
       "\n",
       "[567 rows x 6 columns], token_table=      Topic      Freq         Term\n",
       "term                              \n",
       "0         3  0.971536       amount\n",
       "22        1  0.872967     campaign\n",
       "4         5  0.696388      concert\n",
       "4         6  0.696388      concert\n",
       "7         2  0.971535         cook\n",
       "1         3  0.569093          day\n",
       "16        1  0.872967      example\n",
       "23        1  0.872967       global\n",
       "12        4  1.017717       google\n",
       "8         2  0.971535       gordon\n",
       "5         1  0.767637        great\n",
       "5         5  0.383819        great\n",
       "2         3  0.971536   increasing\n",
       "13        4  1.017717  introducing\n",
       "24        1  0.872967       launch\n",
       "9         2  0.971535         love\n",
       "14        4  1.017717          new\n",
       "3         1  0.533788    pollution\n",
       "3         3  0.533788    pollution\n",
       "17        1  0.872967      present\n",
       "10        2  0.971535       ramsay\n",
       "18        1  0.872967        robot\n",
       "11        2  0.971535          see\n",
       "21        6  1.219207      singing\n",
       "25        1  0.872967         stop\n",
       "15        4  1.017717    technlogy\n",
       "19        1  0.872967   technology\n",
       "20        1  0.872967        today\n",
       "6         5  1.090287           wa\n",
       "26        1  0.872967      warming, R=27, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 9, 20, 4, 10, 3, 15, 14, 13, 12, 11, 19, 16, 8, 7, 17, 5, 18, 2, 1])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, mycorpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
